<!DOCTYPE html>
<html>
<head>
    <script src="settings.js" type="text/javascript"></script>
    <style type="text/css">
        
        /* make chat window fill frame */
        div#webchat > div{
            position: absolute;
            top: 50px;
            right: 0px;
            bottom: 0px;
            left: 0px;
        }

        div#titlebar {
            position: absolute;
            top: 0px;
            right: 0px;
            bottom: 550px;
            left: 0px;
            background: #CCC;
            color: #000;
            font-family: Calibri, "Helvetica Neue", Arial, sans-serif;
        }

        div#title {
            position: absolute;
            padding-left: 1em;
            top: 50%;
            transform: translateY(-50%);
        }

        div#speech {
            position: absolute;
            padding-right: 1em;
            top: 50%;
            transform: translateY(-50%);
            text-align: right;
            right: 0px;
        }

        /* don't display upload file button */
        /* WebChat has an option to turn this off, but it doesn't seem to work */
        div .main > div:first-child {
            display: none;
        }

        /* use pointer cursor over linked images */
        div[role="button"] > img, label {cursor: pointer;}

    </style>
</head>
<body>
    <div id="titlebar">
        <div id="title"><b>Ask J. Edgar Hoover</b></div>
        <div id="speech"><label><input type="checkbox" onclick="speechtoggle(this)" id="toggle" />Use speech</label></div>
    </div>

    <div id="webchat" role="main"></div>
    <script src="https://cdn.botframework.com/botframework-webchat/4.2.0/webchat.js"></script>
    <script src="microsoft.cognitiveservices.speech.sdk.bundle-min.js"></script>
    <script>
        const user = { id: "u" + Date.now(), name: "User", role: "user" };
        const bot = { id: 'hooverbot', name: 'J. Edgar Hoover', role: "bot"};

        const keyword = "Mr. Hoover";
        const kwrx = RegExp("^" + keyword.replace(".", "\\.") + "[ ,]+");

        const connection = window.WebChat.createDirectLine({
            token: botSecret,
        });

        const webchat = document.getElementById('webchat');

        const chat = window.WebChat.renderWebChat({
            bot: bot,
            directLine: connection,
            user: user,
            sendTimeout: 60000, // ms
        }, webchat);
  
        // subscribe to messages so we can speak them
        connection.activity$
            .filter(activity => activity.type === "message" && activity.speak)
            .subscribe(activity => say(activity.speak, activity["cache-speech"]));

        // Web Chat only sends ConversationUpdate after the first message is sent.
        // So this event is only sent to force ConversationUpdate to be set. (Our bot doesn't do anything with it)
        connection.postActivity({
            type: 'event',
            name: 'greetings',
            text: 'greetings',
            value: 'greetings',
            from: user,
         // membersAdded: [user],
        }).subscribe();

        // keep input focused inside the bot entry field
        const field = document.forms[0][0];
        const focus = _ => (window.getSelection().type != "Range") && (field.focus() && window.getSelection().empty());
        focus();
        // focusing periodically turns out to be the most robust method of keeping the input field focused
        setInterval(focus, 250);

        var audioConfig = null;
        var speechConfig = null;
        var speechRecognizer = null;

        var greeted = false;

        // reset Speech toggle, as browsers may retain this across reloads
        document.getElementById("toggle").checked = false;

        // called when speech checkbox is toggled
        function speechtoggle(element) {
            window.getSelection().empty();
            if (element.checked) {
                unlockAudio();
                if (speechRecognizer == null) {
                    var audioConfig = SpeechSDK.AudioConfig.fromDefaultMicrophoneInput();
                    speechConfig = SpeechSDK.SpeechConfig.fromEndpoint(new URL(speechRecognitionEndpoint), speechKey);
                    speechRecognizer = new SpeechSDK.SpeechRecognizer(speechConfig, audioConfig);

                    // called when a complete utterance has been recognized
                    speechRecognizer.recognized = function (sender, event) {
                        var text = event.result.text;
                        // is utterance addressed to Mr. Hoover?
                        var match = text.match(kwrx);
                        if (match) {
                            text = text.slice(match[0].length).trim();
                            if (text.length) {
                                var oldtext = field.value;
                                field.value = text;
                                connection.postActivity({
                                    type: 'message',
                                    text: text,
                                    from: user,
                                }).subscribe(_ => setTimeout(_ => field.value = oldtext, 500));
                            }
                        }
                    }
                    // speech recognition times out after 20 seconds of silence; turn off checkbox
                    // (we do this by sending it a click to make sure the handler runs)
                    speechRecognizer.speechEndDetected = _ => element.checked && element.click();
                    focus();
                }
                // there doesn't seem to be a way to detect whether the speech recognizer was actually able to start up
                // (it will log an error in the console if it can't get the mic, but does not call the error handler)
                // so we have to assume it started up correctly. Display helpful message under bot name.
                document.getElementById("title").innerHTML += '<br><small>Address spoken requests to "' + keyword + '."</small>';
                say = speak;
                speechRecognizer.startContinuousRecognitionAsync();
                if (!greeted) say("Welcome to The JFK Files, I'm J. Edgar Hoover.");
                greeted = true;
            } else {
                speechRecognizer.stopContinuousRecognitionAsync();
                document.getElementById("title").innerHTML = document.getElementById("title").innerHTML.split("<br>")[0];
                say = dontspeak;
            }
        }

        // play a zero-length file to unlock audio. must be called on a user-initiated thread (e.g. UI event handler)
        async function unlockAudio() {
            var player = new Audio(silentaudio);
            player.autoplay = false;
            player.muted = true;
            try {
                var promise = player.play();
                // we may get a DOMError on play (saw this in Chrome) but it'll still work
                if (promise) promise.catch(_ => null);  // might or might not be a promise
            } catch (e) { };
        }

        // use separate queues to handle speech requests and playback of returned audio files
        var token = null;
        var requests = [];
        var playlist = [];

        // cache for audio objects so we don't repeatedly request the same text from Speech Service
        var audcache = {};

        // flags used to manage automatic start of playback queues on call to speak()
        var loading = false;
        var playing = false;
        
        // manage the speech queues, play the audio files, handle token refresh as needed
        function speak(text, cacheable) {
            if (!token) {
                var req = new XMLHttpRequest();
                req.open("POST", tokenEndpoint, true);
                req.setRequestHeader("Ocp-Apim-Subscription-Key", speechKey);
                req.setRequestHeader("Content-type", "application/x-www-form-urlencoded");
                req.onload = function () {
                    // invalidate the token in 9 min. we could actually refresh it at that time,
                    // but if we're not speaking then, it would be unnecessary network access
                    setTimeout(_ => token = null, 9 * 60 * 1000);   
                    token = req.response;
                    enqueue();
                }
                req.send();
            }
            else enqueue();
            return;

            function enqueue() {
                if (audcache[text] !== undefined) {
                    playlist.push(audcache[text]);
                    return startplaying();
                }
                var req = new XMLHttpRequest();
                req.responseType = "blob";
                req.open("POST", speechSynthesisEndpoint, true);
                req.setRequestHeader("Authorization", token);
                req.setRequestHeader("Content-type", "text/plain");     // we're not using SSML
                // we can't play until we've received tho whole file, so use the smallest file size possible
                req.setRequestHeader("X-Microsoft-OutputFormat", "audio-16khz-32kbitrate-mono-mp3");
                req.onload = (function () {
                    var bloburl = URL.createObjectURL(req.response);
                    var player = new Audio();
                    player.autoplay = false;
                    player.src = bloburl;
                    player.onended = function () {
                        playlist.shift();
                        playnext();
                    }
                    if (cacheable === true) audcache[text] = player;
                    playlist.push(player);
                    startplaying();
                    requests.shift();
                    nextreq();
                });
                requests.push(_ => req.send(text));
                if (!loading) nextreq();
                return;

                function nextreq() {
                    requests.length && requests[0]();
                    loading = requests.length > 0;
                }

                function playnext() {
                    playlist.length && playlist[0].play();
                    playing = playlist.length > 0;
                    if (!playing) location.protocol == "https:" && speechRecognizer.startContinuousRecognitionAsync();
                }

                function startplaying() {
                    if (!playing) {
                        greeted && location.protocol == "https:" && speechRecognizer.stopContinuousRecognitionAsync();
                        playnext();
                    }
                }
            }
        }

        // null speech function for when speech is disabled
        function dontspeak(text, cacheable) { };   // don't tell me 'cause it hurts

        var say = dontspeak;

        const silentaudio = "data:audio/mp3;base64,SUQzBAAAAAAAI1RTU0UAAAAPAAADTGF2ZjU2LjM2LjEwMAAAAAAAAAAAAAAA//OEAAAAAAAAAAAAAAAAAAAAAAAASW5mbwAAAA8AAAAEAAABIADAwMDAwMDAwMDAwMDAwMDAwMDAwMDAwMDV1dXV1dXV1dXV1dXV1dXV1dXV1dXV1dXV6urq6urq6urq6urq6urq6urq6urq6urq6v////////////////////////////////8AAAAATGF2YzU2LjQxAAAAAAAAAAAAAAAAJAAAAAAAAAAAASDs90hvAAAAAAAAAAAAAAAAAAAA//MUZAAAAAGkAAAAAAAAA0gAAAAATEFN//MUZAMAAAGkAAAAAAAAA0gAAAAARTMu//MUZAYAAAGkAAAAAAAAA0gAAAAAOTku//MUZAkAAAGkAAAAAAAAA0gAAAAANVVV";

    </script>
</body>
</html>
